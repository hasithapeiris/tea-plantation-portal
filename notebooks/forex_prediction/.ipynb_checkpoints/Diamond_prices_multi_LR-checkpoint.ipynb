{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'diamonds.csv' is not one of the example datasets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load the Diamonds dataset from seaborn\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiamonds.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Display first few rows before preprocessing\u001b[39;00m\n\u001b[0;32m     13\u001b[0m display(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:573\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(name, cache, data_home, **kws)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(cache_path):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m get_dataset_names():\n\u001b[1;32m--> 573\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not one of the example datasets.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    574\u001b[0m     urlretrieve(url, cache_path)\n\u001b[0;32m    575\u001b[0m full_path \u001b[38;5;241m=\u001b[39m cache_path\n",
      "\u001b[1;31mValueError\u001b[0m: 'diamonds.csv' is not one of the example datasets."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Diamonds dataset from seaborn\n",
    "df = sns.load_dataset(\"diamonds.csv\")\n",
    "\n",
    "# Display first few rows before preprocessing\n",
    "display(df.head())\n",
    "\n",
    "# Drop irrelevant columns and handle missing values\n",
    "df.dropna(inplace=True)  # Removing missing values\n",
    "\n",
    "# Manually define categorical features (ensure they are correctly detected)\n",
    "categorical_features = [\"cut\", \"color\", \"clarity\"]  # Explicitly specifying categorical features\n",
    "numerical_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.drop(\"price\")\n",
    "\n",
    "# Convert categorical features to string (if they were mistakenly numerical)\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Print detected categorical features\n",
    "print(\"Categorical features detected:\", categorical_features)\n",
    "\n",
    "# Verify unique values in categorical features\n",
    "print(\"Unique categories per feature before encoding:\")\n",
    "for col in categorical_features:\n",
    "    print(f\"{col}: {df[col].unique()}\")\n",
    "\n",
    "# Print the number of samples and features before encoding and scaling\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of features before encoding and scaling: {df.shape[1] - 1}\")  # Excluding target variable\n",
    "\n",
    "\"\"\"\n",
    "# TODO: Task 1 - Uncomment the following section to apply One-Hot Encoding and Feature Scaling\n",
    "# Instructions:\n",
    "# - Uncomment the code below.\n",
    "# - Run the program and compare results with and without encoding/scaling.\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Encode categorical variables using One-Hot Encoding\n",
    "encoder = OneHotEncoder(drop=\"first\", sparse_output=False)  # Drop first category to avoid redundancy.\n",
    "categorical_encoded = encoder.fit_transform(df[categorical_features])\n",
    "\n",
    "# Convert encoded features into a DataFrame with proper column names\n",
    "categorical_encoded_df = pd.DataFrame(categorical_encoded, columns=encoder.get_feature_names_out(input_features=categorical_features))\n",
    "\n",
    "# Print the number of encoded features after dropping the first category\n",
    "print(f\"Number of encoded features after dropping first category: {categorical_encoded_df.shape[1]}\")\n",
    "\n",
    "# Standardize numerical features to bring them to the same scale\n",
    "scaler = StandardScaler()\n",
    "numerical_scaled = scaler.fit_transform(df[numerical_features])\n",
    "numerical_scaled_df = pd.DataFrame(numerical_scaled, columns=numerical_features)\n",
    "\n",
    "# Combine processed features into a single dataset\n",
    "X = pd.concat([numerical_scaled_df, categorical_encoded_df], axis=1)\n",
    "\"\"\"\n",
    "\n",
    "# ---- TEMPORARY: Using raw numerical data ----\n",
    "# TODO: Task 1: Comment the following line to use the encoded and scaled features\n",
    "# Since encoding and scaling are commented out, we will use only numerical features without transformation.\n",
    "X = df[numerical_features]  # Directly using numerical features without transformation\n",
    "\n",
    "# Target variable\n",
    "y = df[\"price\"]\n",
    "\n",
    "# Print the number of features after skipping encoding and scaling\n",
    "print(f\"Number of features used in model: {X.shape[1]}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Multiple Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "# TODO: Task 2 - Uncomment the following section to apply Ridge Regularization (L2)\n",
    "# Instructions:\n",
    "# - Uncomment the code below.\n",
    "# - Compare results with and without regularization.\n",
    "# - Adjust the alpha parameter to see how regularization strength affects performance.\n",
    "\n",
    "alpha_value = 1  # Regularization strength, can be adjusted\n",
    "ridge_model = Ridge(alpha=alpha_value)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set using Ridge Regression\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "\"\"\"\n",
    "\n",
    "# Predict on test set using Linear Regression\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (Linear Regression): {mse:.2f}\")\n",
    "print(f\"R-squared Score (Linear Regression): {r2:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "# TODO: Task 2 - Evaluate Ridge Regression\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Mean Squared Error (Ridge Regression, alpha={alpha_value}): {mse_ridge:.2f}\")\n",
    "print(f\"R-squared Score (Ridge Regression, alpha={alpha_value}): {r2_ridge:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5, label=\"Linear Regression\", color=\"blue\")\n",
    "\n",
    "\"\"\"\n",
    "# TODO: Task 2 - Add Ridge Regression to the plot\n",
    "sns.scatterplot(x=y_test, y=y_pred_ridge, alpha=0.5, label=\"Ridge Regression\", color=\"red\")\n",
    "\"\"\"\n",
    "\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual vs Predicted Prices\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---- LAB EXERCISE INSTRUCTIONS ----\n",
    "# Step 1: Run the code as it is and note the model performance.\n",
    "# Step 2: Uncomment Task 1 (Encoding and Scaling), run again, and compare results.\n",
    "# Step 3: Uncomment Task 2 (Ridge Regularization), run again, and compare results.\n",
    "# Step 4: Adjust the alpha value in Ridge Regression to observe its effect on performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
